[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ana Jaramillo’s Website",
    "section": "",
    "text": "I am a current Biology major and Mathematics & Statistics minor at Pomona College. This is my landing page for data science, bioinformatics, and computing projects."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Ana Jaramillo, a dedicated Biology major and Mathematics & Statistics minor at Pomona College. I am deeply interested in the scientific research process, but have started to focus my research specialty on data analysis and computational biology. I’ve developed statistical models, analyzed large genomic datasets, and collaborated across disciplines to uncover insights into complex biological systems–such as discovering drug targets for parasitic diseases and differential bacterial/fungal microbial analyses. Through my work, I aim to bridge the worlds of computation and biological research, uncovering new knowledge and contributing to meaningful change in healthcare and genomic computational biology.\n\n\nI am currently working on a research manuscript for my senior thesis, which I hope to publish upon completion. My project focuses on investigating differential microbial assemblages in the soil beneath five native California sagescrub plant species. This research explores the microbial diversity unique to each plant species and investigates how proximity to urban boundaries—an aspect known as the “edge effect”—influences the composition of these microbial communities. By examining edge and central soil samples, I aim to uncover how human-influenced environmental factors might alter microbial ecosystems in these natural settings.\nTo achieve this, I am conducting a bioinformatics analysis on the genomic data of both fungal and bacterial assemblages. Working with tools such as Jupyter Notebook and Python, I process and analyze large genomic data sets to identify patterns and draw meaningful insights. This analysis involves a variety of computational techniques, including data cleaning, genomic sequencing alignment, and community composition visualization, providing me with hands-on experience in advanced bioinformatics workflows."
  },
  {
    "objectID": "TidyTuesdays.html",
    "href": "TidyTuesdays.html",
    "title": "TidyTuesdays",
    "section": "",
    "text": "something in here"
  },
  {
    "objectID": "TT2.html",
    "href": "TT2.html",
    "title": "TidyTuesday #2",
    "section": "",
    "text": "The data this week comes from The Humane League’s US Egg Production dataset by Samara Mendez. Dataset and code is available for this project on OSF at US Egg Production Data Set.\nThis plot explores the egg-laying productivity of hens during the year 2020, based on data from The Humane League’s US Egg Production dataset, curated by Samara Mendez for this TidyTuesday project.\n\n# Packages\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Data\neggproduction  &lt;- \n  readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')\n\n\n# Filter by year\neggproduction |&gt;\n  select(observed_month, n_hens, n_eggs) |&gt;\n  filter(str_starts(as.character(observed_month), \"2020\"))\n\n# A tibble: 48 × 3\n   observed_month   n_hens     n_eggs\n   &lt;date&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n 1 2020-01-31     64019000 1221400000\n 2 2020-02-29     64384000 1154100000\n 3 2020-03-31     64730000 1239900000\n 4 2020-04-30     65044000 1203500000\n 5 2020-05-31     64481000 1239100000\n 6 2020-06-30     64251000 1207400000\n 7 2020-07-31     64353000 1255200000\n 8 2020-08-31     64223000 1257100000\n 9 2020-09-30     63975000 1219800000\n10 2020-10-31     63728000 1254800000\n# ℹ 38 more rows\n\n# Plotting\nggplot(eggproduction, aes(x = observed_month, y = n_eggs/n_hens)) +\n  geom_point() +\n  labs(\n    title = \"Number of eggs laid per hen in 2020\",\n    subtitle = \"*Assuming every hen laid the same number of eggs\",\n    x = \"Date\",\n    y = \"Number of Eggs\"\n  )\n\n\n\n\n\n\n\n\nThe graph represents the average number of eggs laid per hen per month, calculated by dividing the total number of eggs produced by the number of hens. Each point corresponds to a specific month, providing a snapshot of hen productivity over the year.\nThe x-axis shows the date, while the y-axis reflects the number of eggs laid per hen. This visualization assumes equal productivity across all hens, offering an aggregated view of egg production trends.\nInsights from this plot can inform discussions about egg production efficiency, seasonal trends in hen productivity, and the potential impact of external factors like feed quality, environmental conditions, or farming practices during 2020. Additionally, the data highlights the scale of egg production in the U.S., contributing to broader conversations about food systems, sustainability, and animal welfare."
  },
  {
    "objectID": "TT1.html",
    "href": "TT1.html",
    "title": "Numbats Plots",
    "section": "",
    "text": "The data this week comes from the Atlas of Living Australia. Thanks to Di Cook for preparing this week’s dataset!\nThis plot visualizes the geographic coordinates of 787 sightings of the Numbat (Myrmecobius fasciatus), an iconic marsupial native to Australia. The data showcases the distribution of this species based on recorded observations.\n\n# Packages\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Data\nnumbats &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-03-07/numbats.csv')\n\n# Filtering Data\nnumbats |&gt;\n  select(decimalLatitude, decimalLongitude, scientificName) |&gt;\n  filter(scientificName == \"Myrmecobius fasciatus\")\n\n# A tibble: 787 × 3\n   decimalLatitude decimalLongitude scientificName       \n             &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;                \n 1           -37.6             146. Myrmecobius fasciatus\n 2           -35.1             150. Myrmecobius fasciatus\n 3           -35               118. Myrmecobius fasciatus\n 4           -34.7             118. Myrmecobius fasciatus\n 5           -34.6             117. Myrmecobius fasciatus\n 6           -34.6             117. Myrmecobius fasciatus\n 7           -34.6             118. Myrmecobius fasciatus\n 8           -34.6             117. Myrmecobius fasciatus\n 9           -34.6             117. Myrmecobius fasciatus\n10           -34.6             117. Myrmecobius fasciatus\n# ℹ 777 more rows\n\n# Making Plots\nggplot(data = numbats, aes(x = decimalLatitude, y = decimalLongitude)) +\n  geom_point() +\n  labs(title = \"Coordinates of 787 Numbats (Myrmecobius fasciatus)\", x = \"Coordinate Latitude\", y = \"Coordinate Longitude\")\n\n\n\n\n\n\n\n\nThe x-axis represents latitude, and the y-axis represents longitude, offering a two-dimensional view of where Numbats have been documented. Numbats are known for their striking striped coats and their ecological role as termite specialists, making this species an essential part of Australia’s biodiversity.\nThrough visualizations like this, we can better understand the spatial patterns of Numbat occurrences, which can be crucial for conservation efforts aimed at protecting their natural habitats. Their distribution is primarily concentrated in specific regions of Australia, which aligns with known ecological and environmental conditions favorable for this species."
  },
  {
    "objectID": "Mini Project 2.html",
    "href": "Mini Project 2.html",
    "title": "Regular Expressions: Analyzing The Office Lines",
    "section": "",
    "text": "Data was sourced from The Office Lines on Kaggle by Fabriziocominetti\nThe Office is a comedy show following the lifestyle of seemingly normal office workers. As the show follows multiple characters, there is some debate as to who exactly the main character is. As such, I have decided to count the amount of words spoken by each character per season to better understand who the show writers center most. Additionally, as a young adult show, there is some occasional cussing which occurs throughout. Here, I have quantified the percentage of the show script which involves cussing to see how much of the show involves potty-mouth humor.\n# Data\nlibrary(tidyverse)\n\nofficelines &lt;- read_csv(\"the-office_lines.csv\")\n\n# Cleaning Data\nofficelines &lt;- officelines |&gt;\nmutate(Line = str_to_lower(Line))\n\nlines &lt;- officelines$Line\nlines &lt;- str_remove_all(lines, \"[[:punct:]]\")\n\nofficelines &lt;- officelines |&gt;\nselect(Character, Line, Season, Episode_Number) |&gt;\nmutate(Line = lines)"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About",
    "section": "",
    "text": "I am currently working on a research manuscript for my senior thesis, which I hope to publish upon completion. My project focuses on investigating differential microbial assemblages in the soil beneath five native California sagescrub plant species. This research explores the microbial diversity unique to each plant species and investigates how proximity to urban boundaries—an aspect known as the “edge effect”—influences the composition of these microbial communities. By examining edge and central soil samples, I aim to uncover how human-influenced environmental factors might alter microbial ecosystems in these natural settings.\nTo achieve this, I am conducting a bioinformatics analysis on the genomic data of both fungal and bacterial assemblages. Working with tools such as Jupyter Notebook and Python, I process and analyze large genomic datasets to identify patterns and draw meaningful insights. This analysis involves a variety of computational techniques, including data cleaning, genomic sequencing alignment, and community composition visualization, providing me with hands-on experience in advanced bioinformatics workflows."
  },
  {
    "objectID": "about.html#research-experience",
    "href": "about.html#research-experience",
    "title": "About",
    "section": "",
    "text": "University of Melbourne Bioinformatics Research Project\nAs a research intern, I immersed myself in a project that examined gene expression patterns in Schistosoma haematobium, a parasitic organism with significant health impacts in endemic regions. This experience challenged me to bridge my biological knowledge with computational analysis, as I developed and implemented over 12 statistical models in R to interpret and visualize genomic data for over 10,000 sequences. I used the HISAT2 alignment program on the Galaxy platform to map gene expression levels across sex chromosomes, an innovative approach that broadened my understanding of bioinformatics workflows. Working alongside graduate and Ph.D. researchers, I gained firsthand experience in interdisciplinary collaboration, where I learned to refine protocols, troubleshoot data issues, and maintain data accuracy across large datasets. Presenting our findings at an international symposium was a transformative moment, teaching me to distill complex data insights into accessible, impactful narratives for a diverse scientific audience.\nSummer Undergraduate Research Program at Pomona College\nThis research experience was pivotal in expanding my understanding of ecological research design and data management. I led the sample collection and protocol design for a project exploring ecological interactions across five Southern Californian ecosystems, emphasizing precision in data collection to minimize variance across sites. I was responsible for transforming raw field data into a meaningful story that could contribute to the scientific community’s understanding of these unique environments. My contribution to our publication in Diversity included not only statistical analysis but also creating detailed data visualizations that underscored key ecological patterns and implications. Presenting our findings at academic conferences reinforced my ability to communicate scientific data visually and verbally, allowing me to advocate for ecological preservation through accessible, evidence-based presentations.\nRobert J. Bernard Field Station Research\nAt the Bernard Field Station, I engaged deeply with research on plant-soil feedback mechanisms and the ecological impact of native and invasive species, a project that aligned with my passion for biodiversity conservation. Working as a Restoration Technician, I conducted methodical data collection across 12 plots, analyzed microbial interactions, and tested soil nutrient levels. This experience brought home the importance of rigorous, repeatable research practices in ecological restoration, where every detail impacts our understanding of ecosystem health. Our team’s publication illuminated the complex interdependencies between plants, soil, and microbial communities, with my contributions to data visualization capturing the intricate relationships that promote ecosystem resilience. This project was instrumental in teaching me the real-world applications of ecological research, from laboratory techniques like DNA extraction to statistical methods that unveil hidden patterns in complex ecological datasets."
  },
  {
    "objectID": "about.html#laboratory-experience",
    "href": "about.html#laboratory-experience",
    "title": "About",
    "section": "",
    "text": "Cell Chemistry and Cell Biology Lab Teaching Assistant\n\nAs a Laboratory Teaching Assistant, I guided over 30 students in mastering core techniques of data analysis, visualization, and lab procedures. In this role, I emphasized precision and analytical thinking, helping students transform raw data into clear, scientifically sound reports. Leading hands-on sessions, I taught methods like western blotting, gel electrophoresis, and spectroscopy, instilling in students the importance of careful data handling. By fostering collaborative learning and critical problem-solving, I helped students build confidence in both their technical skills and their ability to interpret and present data—a foundation essential for future research and professional success.\nSkills: Western blotting, SDS-Page, agarose gel electrophoresis, spectroscopy, PCR, DNA Sequencing, DNA extraction & purification, immunofluorescence, cell culturing, transfection and transformation, knockout techniques, co-immunoprecipitation, spectroscopy, distillation, enzyme kinetic assays"
  },
  {
    "objectID": "about.html#honors-and-scholarships",
    "href": "about.html#honors-and-scholarships",
    "title": "About",
    "section": "",
    "text": "Posse Foundation Full-Tuition Leadership Scholarship (2021 - 2025) 1for2 Foundation “Pay-it-Forward” Scholarship ($14,700 – 2021-2025) Institute for Study Abroad Scholarship—Melbourne, Australia: Academic Year (2023) Taco Bell Live Mas Video Scholarship ($10,000 – 2021; $10,000 – 2023) 2 Published Research Articles (2022 – 2023) Hispanic Scholarship Fund Scholar (2020 – 2023)"
  },
  {
    "objectID": "Mini Project 3.html",
    "href": "Mini Project 3.html",
    "title": "Real Estate Simulation",
    "section": "",
    "text": "Data was sourced from Real Estate Valuation from UC Irvine Machine Learning Repository by I-Cheng Yeh.\nThis dataset provides insights into the historical market of real estate valuation collected from Sindian Dist., New Taipei City, Taiwan. It includes many interesting variables for comparison, such as features of homes and their potential association with valuation. In particular, the distance from homes to the nearest MRT station stood out, as Taiwan’s extensive public transportation system, including its popular MRT, plays a significant role in daily life.\nI was interested in examining whether there is a significant association between the distance of homes from the nearest MRT station and house price per unit area. While one might hypothesize that proximity to an MRT station increases valuation, the dataset itself does not provide evidence for causal claims—our analysis focuses solely on the association between these variables.\nResearch Question: Is there a significant association between distance from the nearest MRT station and house price per unit area in Taiwan?\nNull Hypothesis: There is no association between the distance to the nearest MRT station and house price per unit area.\nAlternative Hypothesis: There is an association between the distance to the nearest MRT station and house price per unit area.\n\nVisualizing the Data\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Data\nreal_estate &lt;- read_excel(\"C:/Users/anaja/Downloads/Real estate valuation data set.xlsx\")\n\nTo start, I plotted the observed relationship between MRT distance and house price per unit area. The slope of the linear regression model appears negative, suggesting that homes closer to MRT stations tend to have higher prices per unit area. However, this visual pattern could occur by chance. To assess the significance of this relationship, I will conduct a permutation test.\n\n# Linear regression of Normal MRT Distance to House Price per Unit Area\nreal_estate |&gt;\n  ggplot(aes(x = distance_MRT, y = house_price_unit)) +\n  geom_point() +\n  labs(x = \"Distance from MRT (meters)\", y = \"House Price of Unit Area\",\n       y = \"House Price of Unit Area\",\n       title = \"MRT Distances to House Price per Unit Area\" ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n# Linear regression of Randomly Sampled MRT Distances to House Price per Unit Area\nset.seed(47)\nreal_estate |&gt;\n    mutate(random_MRT = sample(distance_MRT)) |&gt;\n    ggplot(aes(x = random_MRT, y = house_price_unit)) +\n    geom_point() +\n  labs(x = \"Distance from MRT (meters)\", y = \"House Price of Unit Area\",\n       title = \"Randomly Sampled MRT Distances to House Price per Unit Area\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nHere, we calculate the observed slope’s coefficient to numerically understand how much of a difference we see in slopes.\n\n# Getting the observed slope for original data and plot\nobserved_model &lt;- lm(house_price_unit ~ distance_MRT, data = real_estate)\nobserved_slope &lt;- coef(observed_model)[[\"distance_MRT\"]]\n\nobserved_slope\n\n[1] -0.007262052\n\n# Running Permutation Test with slope for Null Hypothesis\nset.seed(47)\n\npermuted_MRT_prices &lt;- function(real_estate) {\n  real_estate &lt;- real_estate |&gt;\n    mutate(random_MRT = sample(distance_MRT))\n  model_random_MRT &lt;- lm(house_price_unit ~ random_MRT, data = real_estate)\n  return(coef(model_random_MRT)[[\"random_MRT\"]])\n}\n\npermuted_MRT_prices(real_estate)\n\n[1] 0.0005357146\n\n\n\n\nConducting a Permutation Test\nA permutation test reshuffles the MRT distances randomly and calculates a new slope each time. By comparing the observed slope from the original data to the distribution of slopes generated from 1000 random permutations, we can evaluate whether the observed relationship is likely to have arisen by chance.\nThe observed slope for the original dataset is -0.0073, while the average slope from the null distribution of 1000 permutations is approximately 0.00002, indicating a near-zero association under the null hypothesis. When plotted, the histogram of null slopes clearly shows that the observed slope (red line) lies far outside the null distribution, suggesting that the observed negative slope is statistically significant.\n\n# Iterating over multiple random samples of MRT distance\nset.seed(47)\n\nn_permutations &lt;- 1000\nnull_slopes &lt;- map_dbl(1:n_permutations, ~ permuted_MRT_prices(real_estate))\n\naverage_sample_slope &lt;- mean(null_slopes)\n\naverage_sample_slope\n\n[1] 2.056867e-05\n\n\nSo over 1000 iterations, we have an average reshuffled slope of:\n2.056867e-05\nThis is compared to our very negative slope of the original dataset:\n -0.007262052\nLets create a histogram of the null statistics and see if our observed values fall within the histogram:\n\ncomparison_slopes &lt;- tibble(null_slopes, observed_slope)\n\nset.seed(47)\nperm_stats &lt;-\n  map(1:1000, null_slopes, data = comparison_slopes) |&gt;\n  list_rbind()\n\nperm_stats |&gt;\n  ggplot(aes(x = null_slopes)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = observed_slope), color = \"red\") +\n               labs(title = \"Histogram of 1000 Slope Permutations\", \n                    subtitle = \"red line = observed slope (from original data)\", x = \"Permuted Null Slopes\", y = \"Count\")\n\n\n\n\n\n\n\n\nIt seems our observed slope (red line) is not near the slopes calculated in 1000 iterations. That’s to say, there is likely no way that our observed data would happen by chance–even if we were to run this simulation 1000 times.\n\n\nStatistical Testing\nTo complement the permutation test, I conducted a linear regression analysis. I reported the p-value associated with the slope of the regression line (not the intercept). The p-value for the slope (4.639825e-56) is essentially 0 (&lt;&lt; 0.05), allowing us to confidently reject the null hypothesis. This result aligns with the permutation test, further supporting the conclusion that there is a statistically significant association between house prices per unit area and the distance to the nearest MRT station.\n\nlibrary(broom)\nt_test &lt;- lm(house_price_unit ~ distance_MRT, data = real_estate) |&gt;\n  tidy()\n\nt_test$p.value\n\n[1] 1.856440e-231  4.639825e-56\n\n\n\n\nResults and Implications\nThe analyses show strong evidence of a negative association between MRT proximity and house price per unit area in Taiwan. While this study does not establish causality, the results suggest that proximity to MRT stations may be an important factor associated with real estate valuation. Future studies could extend this analysis to other regions or include additional variables to better understand the dynamics of Taiwan’s real estate market.\nCitation:\nYeh, I. (2018). Real Estate Valuation [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5J30W."
  },
  {
    "objectID": "Mini Project 4.html",
    "href": "Mini Project 4.html",
    "title": "SQL Data Analysis",
    "section": "",
    "text": "This project utilizes the Wideband Acoustic Immittance (WAI) Database hosted by Smith College to explore data analysis using SQL and R. Specifically, I reproduced Figure 1 from the Voss (2020) publication and created a visualization comparing mean absorbance of sound among subjects of different races from one study in the database. The dataset was explored and manipulated in SQL, while R was used for visualization.\nFigure 1 Voss (2020)\n\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nPART 1: Recreating Voss (2020) Figure 1\nIn this part of my work, I recreated the Voss (2020) Figure 1 as closely as possible by utilizing ggplot2. I began by selecting my relevant variables, and joining via primary keys. I then isolated the data for the studies included in the plot, the desired frequencies, and grouped by the relevant variables. NOTE: Some deviation from the original figure results from updates in the dataset since the figure publication in 2020–4 years of additional data is included.\n\nSELECT \np.Identifier,\np.AuthorsShortList,\np.Year,\nm.Instrument,\nCOUNT(DISTINCT CONCAT(m.SubjectNumber, m.Ear)) AS Unique_Ears,\nm.Frequency, \nAVG(m.Absorbance) AS Mean_Absorbance\nFROM Measurements AS m \nJOIN PI_Info AS p\nON m.Identifier = p.Identifier\nJOIN Subjects AS s ON m.SubjectNumber = s.SubjectNumber\nWHERE p.Identifier IN ('Abur_2014', 'Feeney_2017', 'Groon_2015',\n'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', \n'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', \n'Werner_2010')\nAND m.Frequency BETWEEN 200 AND 8000\nGROUP BY \np.Identifier,\np.AuthorsShortList,\np.Year, m.Instrument,\nm.Frequency\nHAVING\nUnique_Ears &gt; 0\nORDER BY\np.Identifier, m.Frequency;\n\n\nfreq_data |&gt;\n  mutate(Label = paste(AuthorsShortList, \"(\", Year, \") N=\", Unique_Ears, \";\", Instrument)) |&gt;\n  group_by(Label) |&gt;\n  arrange(desc(Label)) |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Label )) +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  scale_x_continuous(limits = c(200,8000), breaks = seq(200, 8000, by = 1000)) +\n  labs(\n    title = \"Mean absorbance from each publication in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\", \n    color = \"Study Details\"\n  ) + \n  theme_minimal()  +\n  scale_color_manual(values = c(\n    \"purple\", \"deepskyblue\", \"aquamarine\", \"darkolivegreen2\", \"darkseagreen1\", \"yellow\", \n    \"khaki2\", \"lemonchiffon2\", \"goldenrod1\", \"orange\", \"sienna2\", \"red\", \"tomato4\"))\n\n\n\n\n\n\n\n\nPART 2: Mean Absorbance of Lewis 2015 Study by Subject Race\nIn this part, I chose to visualize the data of the Lewis 2015 study whose data was collected within the WAI Database. In this, I grouped the data by subject race in visualization, maintaining the previous frequency threshold. I started by creating a SQL subquery for the data I desired, and visualized the mean absorbency to frequency of subjects amongst different racial groups\n\nSELECT \ns.Race, \np.Identifier,\nCOUNT(DISTINCT CONCAT(m.SubjectNumber, m.Ear)) AS Unique_Ears,\nm.Frequency, \nAVG(m.Absorbance) AS Mean_Absorbance\nFROM Measurements AS m \nJOIN PI_Info AS p\nON m.Identifier = p.Identifier\nJOIN Subjects AS s ON m.SubjectNumber = s.SubjectNumber\nWHERE \np.Identifier = ('Lewis_2015')\nAND m.Frequency BETWEEN 200 AND 8000\nGROUP BY s.Race, p.Identifier, m.Frequency\nHAVING\nUnique_Ears &gt; 0;\n\n\nrace_data &lt;- race_data |&gt; \n  mutate(Race = str_trim(Race),   \n         Race = str_to_title(Race)) \n\nrace_data |&gt; \n  group_by(Race) |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Race)) +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  scale_x_continuous(limits = c(200, 8000), breaks = seq(200, 8000, by = 1000)) +\n  labs(\n    title = \"Mean absorbance by Subject Race in Lewis 2015 Data\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\", \n    color = \"Test Subject Race\",\n    caption = \"Recreating Voss (2020) Figure\"\n  ) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nTo conclude, I created subqueries of the WAI Database to best manipulate the data into visualizations using ggplot2. Although the data originated in SQL form, SQL was used to create subqueries that translate into a dataframe output in R. This was then used to create graphs via ggplot2 and tidyverse functions. In the end, a graph mimicking the output of Voss (2020) Figure 1 (with slight, noted deviations) and a graph to visualize another aspect of a specific study, Lewis 2015, differences in mean absorbency at different frequencies among subject race.\nCitation:\nVoss, Susan E., et al. “Wideband Acoustic Immittance Dataset.” Api.Openalex.Org, Smith College, api.openalex.org/works/doi:10.3837/tiis.2022.12.001. Accessed 28 Nov. 2024.\ndoi.org/10.35482/egr.001.2022\nVoss, Susan E. “An Online Wideband Acoustic Immittance (WAI) Database and Corresponding Website.” Ear and Hearing, U.S. National Library of Medicine, 1 Nov. 2020, pmc.ncbi.nlm.nih.gov/articles/PMC7093226/."
  },
  {
    "objectID": "Final Presentation.html#data-analyses-of-the-office",
    "href": "Final Presentation.html#data-analyses-of-the-office",
    "title": "Final Presentation",
    "section": "Data Analyses of The Office",
    "text": "Data Analyses of The Office\nToday, I will be presenting to you (NBC Marketers) some data from The Office that will be used to consider future directions for an upcoming season. With this, we hope to inform show producers’ decisions and increase The Office’s potential viewership."
  },
  {
    "objectID": "Final Presentation.html#the-office-background",
    "href": "Final Presentation.html#the-office-background",
    "title": "Final Presentation",
    "section": "The Office: Background",
    "text": "The Office: Background\n\nDescriptionThe Office is a comedy mockumentary following the lifestyle of seemingly normal office workers at a branch of Dunder Mifflin Paper Company. It is an American show based off the original British version under the same name, adapted for NBC by Greg Daniels."
  },
  {
    "objectID": "Final Presentation.html#the-office-viewership",
    "href": "Final Presentation.html#the-office-viewership",
    "title": "Final Presentation",
    "section": "The Office Viewership",
    "text": "The Office Viewership\nConsiderations:\n\nCharacter Popularity\nLine Designation for Future Seasons\nExisting Trends"
  },
  {
    "objectID": "Final Presentation.html#who-is-the-favorite-character",
    "href": "Final Presentation.html#who-is-the-favorite-character",
    "title": "Final Presentation",
    "section": "Who is the Favorite Character?",
    "text": "Who is the Favorite Character?\nRanker.com\nA platform for user-generated rankings on various topics, including entertainment.\n\nIt seems Michael Scott has the public vote."
  },
  {
    "objectID": "Final Presentation.html#method-for-main-character-determination",
    "href": "Final Presentation.html#method-for-main-character-determination",
    "title": "Final Presentation",
    "section": "Method for Main Character Determination",
    "text": "Method for Main Character Determination\nAs the show follows multiple characters, there is some debate as to who exactly the main character is.\n\nMethod:\n\nCount the amount of words spoken by each character per season\nUnderstand who the show writers center most"
  },
  {
    "objectID": "Final Presentation.html#data",
    "href": "Final Presentation.html#data",
    "title": "Final Presentation",
    "section": "Data",
    "text": "Data\n\n# Data \nlibrary(tidyverse)\n\nofficelines &lt;- read_csv(\"the-office_lines.csv\")  \nhead(officelines)\n\n# A tibble: 6 × 5\n   ...1 Character Line                                     Season Episode_Number\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                                     &lt;dbl&gt;          &lt;dbl&gt;\n1     0 Michael   All right Jim. Your quarterlies look ve…      1              1\n2     1 Jim       Oh, I told you. I couldn’t close it. So…      1              1\n3     2 Michael   So you’ve come to the master for guidan…      1              1\n4     3 Jim       Actually, you called me in here, but ye…      1              1\n5     4 Michael   All right. Well, let me show you how it…      1              1\n6     5 Michael   [on the phone] Yes, I’d like to speak t…      1              1"
  },
  {
    "objectID": "Final Presentation.html#homogenizing-line-data",
    "href": "Final Presentation.html#homogenizing-line-data",
    "title": "Final Presentation",
    "section": "Homogenizing Line Data",
    "text": "Homogenizing Line Data\n  \n\n# Cleaning Data\nofficelines &lt;- officelines |&gt; \n  mutate(Line = str_to_lower(Line))  \nlines &lt;- officelines$Line \nlines &lt;- str_remove_all(lines, \"[[:punct:]]\")  \nofficelines &lt;- officelines |&gt; \n  select(Character, Line, Season, Episode_Number) |&gt; \n  mutate(Line = lines)   \n\n# Homogenizing the lines \nwords_per_char &lt;- officelines |&gt; \n  mutate(word_count = lengths(str_split(Line, \"\\\\s+\"))) |&gt; \n  group_by(Character, Season) |&gt; \n  summarize(total_words = sum(word_count))"
  },
  {
    "objectID": "Final Presentation.html#most-relevant-characters",
    "href": "Final Presentation.html#most-relevant-characters",
    "title": "Final Presentation",
    "section": "Most Relevant Characters",
    "text": "Most Relevant Characters\n\nSelected via The Office Wiki"
  },
  {
    "objectID": "Final Presentation.html#cleaning-character-list",
    "href": "Final Presentation.html#cleaning-character-list",
    "title": "Final Presentation",
    "section": "Cleaning Character List",
    "text": "Cleaning Character List\n\ndistinct_char &lt;- words_per_char |&gt;\n  distinct(Character)\n\ndistinct_char\n\n# A tibble: 776 × 1\n# Groups:   Character [776]\n   Character           \n   &lt;chr&gt;               \n 1 (Pam’S Mom) Heleen  \n 2 3Rd Athlead Employee\n 3 4Th Athlead Employee\n 4 A.J.                \n 5 Aaron Rodgers       \n 6 Abby                \n 7 Abe                 \n 8 Actor               \n 9 Actress             \n10 Ad Guy 1            \n# ℹ 766 more rows\n\n\nCharacters like “Pam’s Mom” need to be removed! We should also should make sure to count things said by main characters together."
  },
  {
    "objectID": "Final Presentation.html#filtering-for-most-relevant-characters",
    "href": "Final Presentation.html#filtering-for-most-relevant-characters",
    "title": "Final Presentation",
    "section": "Filtering for Most Relevant Characters",
    "text": "Filtering for Most Relevant Characters\n\n#Keeping only Main Characters & Cleaning Data\nmain_characters &lt;- c(\"Michael\", \"Jim\", \"Pam\", \"Dwight\", \"Andy\", \"Ryan\", \"Robert\")\n\nwords_dont_keep &lt;- c(\"Voicemail\", \"Mom\", \"Dad\", \"Ad\", \"Fake\", \"Except\", \"Church\")\n\nword_pattern_keep &lt;- paste0(\"\\\\b(\", paste(main_characters, collapse = \"|\"), \")\\\\b\")\n\nword_pattern_delete &lt;- paste0(\"\\\\b(\", paste(words_dont_keep, collapse = \"|\"), \")\\\\b\")\n\nget_rid_ands &lt;- \"\\\\s&\\\\s|\\\\sAnd\\\\s|/|,\\\\s\"\n\nwords_per_char &lt;- words_per_char |&gt;\nfilter(str_detect(Character, word_pattern_keep)) |&gt;\nfilter(!str_detect(Character, word_pattern_delete)) |&gt;\nmutate(Character = str_replace_all(Character, get_rid_ands, \", \")) |&gt;\nseparate_rows(Character, sep = \", \") |&gt;\nfilter(str_detect(Character, word_pattern_keep)) |&gt;\nmutate(Character = case_when(\nstr_detect(Character, \"\\\\bJim\\\\b\") ~ \"Jim\",\nstr_detect(Character, \"\\\\bPam\\\\b\") ~ \"Pam\",\nstr_detect(Character, \"\\\\bDwight\\\\b\") ~ \"Dwight\",\nstr_detect(Character, \"\\\\bMichael\\\\b\") ~ \"Michael\",\nstr_detect(Character, \"\\\\bAndy\\\\b\") ~ \"Andy\",\nstr_detect(Character, \"\\\\bRobert\\\\b\") ~ \"Robert\", \nstr_detect(Character, \"\\\\bRyan\\\\b\") ~ \"Ryan\")) |&gt;\ngroup_by(Character, Season) |&gt;\nsummarise(total_words = sum(total_words), .groups = 'drop')"
  },
  {
    "objectID": "Final Presentation.html#final-dataset",
    "href": "Final Presentation.html#final-dataset",
    "title": "Final Presentation",
    "section": "Final Dataset",
    "text": "Final Dataset\n\nwords_per_char\n\n# A tibble: 55 × 3\n   Character Season total_words\n   &lt;chr&gt;      &lt;dbl&gt;       &lt;int&gt;\n 1 Andy           3        4867\n 2 Andy           4        2924\n 3 Andy           5        5624\n 4 Andy           6        6776\n 5 Andy           7        6611\n 6 Andy           8       14452\n 7 Andy           9        9157\n 8 Dwight         1        3699\n 9 Dwight         2       11895\n10 Dwight         3       10226\n# ℹ 45 more rows"
  },
  {
    "objectID": "Final Presentation.html#data-visualization",
    "href": "Final Presentation.html#data-visualization",
    "title": "Final Presentation",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nggplot(words_per_char, aes(x = Character, y = total_words, fill = Character)) +\ngeom_bar(stat = \"identity\") +\nfacet_wrap(~Season) +\ntheme_minimal() +\nlabs(\ntitle = \"Who says the most words in The Office?\",\nsubtitle = \"Main characters selected via The Office Wiki\",\nx = \"Season\", \ny = \"Total Words Spoken\") +\n  theme(axis.text.x = element_blank())"
  },
  {
    "objectID": "Final Presentation.html#takeaways",
    "href": "Final Presentation.html#takeaways",
    "title": "Final Presentation",
    "section": "Takeaways",
    "text": "Takeaways\n\nMichael Scott’s Dominance in Word Count:\n\nMichael consistently speaks the most words across all seasons where he appears.\n\nDrop in Michael’s Words After Season 7:\n\nThere is a sharp decline in Michael’s dialogue starting in Season 8 (after his departure).\nOther characters, such as Andy, Dwight, and Jim, seem to fill the void, but none fully replace his prominence.\n\nIncreased Focus on Other Characters in Later Seasons:\n\nAndy’s role expands significantly in Seasons 8 and 9.\nJim and Dwight play a strong secondary role."
  },
  {
    "objectID": "Final Presentation.html#main-character-and-trend",
    "href": "Final Presentation.html#main-character-and-trend",
    "title": "Final Presentation",
    "section": "Main Character and Trend",
    "text": "Main Character and Trend\nTo ensure the success of Season 10, producers should strongly consider bringing Michael Scott back as a central character. Michael Scott was not only a pivotal figure in the show’s narrative but also a driving force behind its immense popularity.\nReinstating Michael Scott as the main character would tap into the nostalgia of long-time fans while reigniting interest among lapsed viewers. By leveraging his role as the heart of the show, NBC can create a revitalized narrative that resonates with both loyal fans and new audiences, ensuring sustained and even increased viewership for the series."
  },
  {
    "objectID": "Final Presentation.html#the-office",
    "href": "Final Presentation.html#the-office",
    "title": "Final Presentation",
    "section": "The Office",
    "text": "The Office\n\nBackgroundThe Office is a comedy mockumentary following the lifestyle of seemingly normal office workers at a branch of Dunder Mifflin Paper Company. It is an American show based off the original British version under the same name, adapted for NBC by Greg Daniels."
  },
  {
    "objectID": "Final Presentation.html#homogenizing-total-line-data",
    "href": "Final Presentation.html#homogenizing-total-line-data",
    "title": "Final Presentation",
    "section": "Homogenizing Total Line Data",
    "text": "Homogenizing Total Line Data\n\n# Cleaning Data\nofficelines &lt;- officelines |&gt;\nmutate(Line = str_to_lower(Line))\n\nlines &lt;- officelines$Line\nlines &lt;- str_remove_all(lines, \"[[:punct:]]\")\n\nofficelines &lt;- officelines |&gt;\nselect(Character, Line, Season, Episode_Number) |&gt;\nmutate(Line = lines) \n\n# Counting the words\nwords_per_char &lt;- officelines |&gt;\nmutate(word_count = lengths(str_split(Line, \"\\\\s+\"))) |&gt;\ngroup_by(Character, Season) |&gt;\nsummarize(total_words = sum(word_count))\n\n\nlowercase\nremove punctuation\nsum words said by character in a season"
  },
  {
    "objectID": "about.html#current-projects",
    "href": "about.html#current-projects",
    "title": "About",
    "section": "",
    "text": "I am currently working on a research manuscript for my senior thesis, which I hope to publish upon completion. My project focuses on investigating differential microbial assemblages in the soil beneath five native California sagescrub plant species. This research explores the microbial diversity unique to each plant species and investigates how proximity to urban boundaries—an aspect known as the “edge effect”—influences the composition of these microbial communities. By examining edge and central soil samples, I aim to uncover how human-influenced environmental factors might alter microbial ecosystems in these natural settings.\nTo achieve this, I am conducting a bioinformatics analysis on the genomic data of both fungal and bacterial assemblages. Working with tools such as Jupyter Notebook and Python, I process and analyze large genomic data sets to identify patterns and draw meaningful insights. This analysis involves a variety of computational techniques, including data cleaning, genomic sequencing alignment, and community composition visualization, providing me with hands-on experience in advanced bioinformatics workflows."
  },
  {
    "objectID": "Mini Project 2.html#presentation",
    "href": "Mini Project 2.html#presentation",
    "title": "Regular Expressions: Analyzing The Office Lines",
    "section": "Presentation",
    "text": "Presentation\nhttps://anasjaramillo.github.io/Final%20Presentation.html#/title-slide"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Feel free to browse or download my resume below to learn more about my academic achievements, professional experiences, and technical skills. For further inquiries or opportunities, don’t hesitate to get in touch through the contact information provided.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"files/Ana_Jaramillo.resume.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  }
]