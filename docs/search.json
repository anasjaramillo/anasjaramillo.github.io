[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ana Jaramillo’s Website",
    "section": "",
    "text": "I am a current Biology major and Mathematics & Statistics minor at Pomona College. This is my landing page for data science, bioinformatics, and computing projects."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Ana Jaramillo, a dedicated Biology major and Mathematics & Statistics minor at Pomona College. My passion for scientific exploration has driven me to work on a range of projects, from bioinformatics research at the University of Melbourne to ecological restoration at the Bernard Field Station. I am deeply interested in the scientific research process, but have started to focus my research specialty on data analysis and computational biology. I’ve developed statistical models, analyzed large genomic datasets, and collaborated across disciplines to uncover insights into complex biological systems–such as discovering drug targets for parasitic diseases and differential bacterial/fungal microbial analyses. As a teaching assistant, I guided students in data analysis and lab techniques, nurturing the next generation of scientific thinkers. Through my work, I aim to bridge the worlds of data and biology, uncovering new knowledge and contributing to meaningful change in healthcare and genomic computational biology.\n\n\nI am currently working on a research manuscript for my senior thesis, which I hope to publish upon completion. My project focuses on investigating differential microbial assemblages in the soil beneath five native California sagescrub plant species. This research explores the microbial diversity unique to each plant species and investigates how proximity to urban boundaries—an aspect known as the “edge effect”—influences the composition of these microbial communities. By examining edge and central soil samples, I aim to uncover how human-influenced environmental factors might alter microbial ecosystems in these natural settings.\nTo achieve this, I am conducting a bioinformatics analysis on the genomic data of both fungal and bacterial assemblages. Working with tools such as Jupyter Notebook and Python, I process and analyze large genomic datasets to identify patterns and draw meaningful insights. This analysis involves a variety of computational techniques, including data cleaning, genomic sequencing alignment, and community composition visualization, providing me with hands-on experience in advanced bioinformatics workflows.\n\n\n\n\nUniversity of Melbourne Bioinformatics Research Project\nAs a research intern, I immersed myself in a project that examined gene expression patterns in Schistosoma haematobium, a parasitic organism with significant health impacts in endemic regions. This experience challenged me to bridge my biological knowledge with computational analysis, as I developed and implemented over 12 statistical models in R to interpret and visualize genomic data for over 10,000 sequences. I used the HISAT2 alignment program on the Galaxy platform to map gene expression levels across sex chromosomes, an innovative approach that broadened my understanding of bioinformatics workflows. Working alongside graduate and Ph.D. researchers, I gained firsthand experience in interdisciplinary collaboration, where I learned to refine protocols, troubleshoot data issues, and maintain data accuracy across large datasets. Presenting our findings at an international symposium was a transformative moment, teaching me to distill complex data insights into accessible, impactful narratives for a diverse scientific audience.\nSummer Undergraduate Research Program at Pomona College\nThis research experience was pivotal in expanding my understanding of ecological research design and data management. I led the sample collection and protocol design for a project exploring ecological interactions across five Southern Californian ecosystems, emphasizing precision in data collection to minimize variance across sites. I was responsible for transforming raw field data into a meaningful story that could contribute to the scientific community’s understanding of these unique environments. My contribution to our publication in Diversity included not only statistical analysis but also creating detailed data visualizations that underscored key ecological patterns and implications. Presenting our findings at academic conferences reinforced my ability to communicate scientific data visually and verbally, allowing me to advocate for ecological preservation through accessible, evidence-based presentations.\nRobert J. Bernard Field Station Research\nAt the Bernard Field Station, I engaged deeply with research on plant-soil feedback mechanisms and the ecological impact of native and invasive species, a project that aligned with my passion for biodiversity conservation. Working as a Restoration Technician, I conducted methodical data collection across 12 plots, analyzed microbial interactions, and tested soil nutrient levels. This experience brought home the importance of rigorous, repeatable research practices in ecological restoration, where every detail impacts our understanding of ecosystem health. Our team’s publication illuminated the complex interdependencies between plants, soil, and microbial communities, with my contributions to data visualization capturing the intricate relationships that promote ecosystem resilience. This project was instrumental in teaching me the real-world applications of ecological research, from laboratory techniques like DNA extraction to statistical methods that unveil hidden patterns in complex ecological datasets.\n\n\n\n\n\nCell Chemistry and Cell Biology Lab Teaching Assistant\n\nAs a Laboratory Teaching Assistant, I guided over 30 students in mastering core techniques of data analysis, visualization, and lab procedures. In this role, I emphasized precision and analytical thinking, helping students transform raw data into clear, scientifically sound reports. Leading hands-on sessions, I taught methods like western blotting, gel electrophoresis, and spectroscopy, instilling in students the importance of careful data handling. By fostering collaborative learning and critical problem-solving, I helped students build confidence in both their technical skills and their ability to interpret and present data—a foundation essential for future research and professional success.\nSkills: Western blotting, SDS-Page, agarose gel electrophoresis, spectroscopy, PCR, DNA Sequencing, DNA extraction & purification, immunofluorescence, cell culturing, transfection and transformation, knockout techniques, co-immunoprecipitation, spectroscopy, distillation, enzyme kinetic assays\n\n\n\nPosse Foundation Full-Tuition Leadership Scholarship (2021 - 2025) 1for2 Foundation “Pay-it-Forward” Scholarship ($14,700 – 2021-2025) Institute for Study Abroad Scholarship—Melbourne, Australia: Academic Year (2023) Taco Bell Live Mas Video Scholarship ($10,000 – 2021; $10,000 – 2023) 2 Published Research Articles (2022 – 2023) Hispanic Scholarship Fund Scholar (2020 – 2023)"
  },
  {
    "objectID": "TidyTuesdays.html",
    "href": "TidyTuesdays.html",
    "title": "TidyTuesdays",
    "section": "",
    "text": "something in here"
  },
  {
    "objectID": "TT2.html",
    "href": "TT2.html",
    "title": "TidyTuesday #2",
    "section": "",
    "text": "Data was sourced from US Egg Production Data by jonthegeek on Github\n\n# Packages\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Data\neggproduction  &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')\n\n\n# Filter by year\neggproduction |&gt;\n  select(observed_month, n_hens, n_eggs) |&gt;\n  filter(str_starts(as.character(observed_month), \"2020\"))\n\n# A tibble: 48 × 3\n   observed_month   n_hens     n_eggs\n   &lt;date&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n 1 2020-01-31     64019000 1221400000\n 2 2020-02-29     64384000 1154100000\n 3 2020-03-31     64730000 1239900000\n 4 2020-04-30     65044000 1203500000\n 5 2020-05-31     64481000 1239100000\n 6 2020-06-30     64251000 1207400000\n 7 2020-07-31     64353000 1255200000\n 8 2020-08-31     64223000 1257100000\n 9 2020-09-30     63975000 1219800000\n10 2020-10-31     63728000 1254800000\n# ℹ 38 more rows\n\n# Plotting\nggplot(eggproduction, aes(x = observed_month, y = n_eggs/n_hens)) +\n  geom_point() +\n  labs(\n    title = \"Number of eggs laid per hen in 2020\",\n    subtitle = \"*Assuming every hen laid the same number of eggs\",\n    x = \"Date\",\n    y = \"Number of Eggs\"\n  )"
  },
  {
    "objectID": "TT1.html",
    "href": "TT1.html",
    "title": "TidyTuesday #1",
    "section": "",
    "text": "Data was sourced from Numbats in Australia by jonthegeek on Github\n\n# Packages\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Data\nnumbats &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-03-07/numbats.csv')\n\n# Filtering Data\nnumbats |&gt;\n  select(decimalLatitude, decimalLongitude, scientificName) |&gt;\n  filter(scientificName == \"Myrmecobius fasciatus\")\n\n# A tibble: 787 × 3\n   decimalLatitude decimalLongitude scientificName       \n             &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;                \n 1           -37.6             146. Myrmecobius fasciatus\n 2           -35.1             150. Myrmecobius fasciatus\n 3           -35               118. Myrmecobius fasciatus\n 4           -34.7             118. Myrmecobius fasciatus\n 5           -34.6             117. Myrmecobius fasciatus\n 6           -34.6             117. Myrmecobius fasciatus\n 7           -34.6             118. Myrmecobius fasciatus\n 8           -34.6             117. Myrmecobius fasciatus\n 9           -34.6             117. Myrmecobius fasciatus\n10           -34.6             117. Myrmecobius fasciatus\n# ℹ 777 more rows\n\n# Making Plots\nggplot(data = numbats, aes(x = decimalLatitude, y = decimalLongitude)) +\n  geom_point() +\n  labs(title = \"Coordinates of 787 Numbats (Myrmecobius fasciatus)\", x = \"Coordinate Latitude\", y = \"Coordinate Longitude\")"
  },
  {
    "objectID": "Mini Project 2.html",
    "href": "Mini Project 2.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Data was sourced from The Office Lines on Kaggle by Fabriziocominetti\nThe Office is a comedy show following the lifestyle of seemingly normal office workers. As the show follows multiple characters, there is some debate as to who exactly the main character is. As such, I have decided to count the amount of words spoken by each character per season to better understand who the show writers center most. Additionally, as a young adult show, there is some occasional cussing which occurs throughout. Here, I have quantified the percentage of the show script which involves cussing to see how much of the show involves potty-mouth humor.\n\n# Data\nlibrary(tidyverse)\n\nofficelines &lt;- read_csv(\"the-office_lines.csv\")\n\n# Cleaning Data\nofficelines &lt;- officelines |&gt;\nmutate(Line = str_to_lower(Line))\n\nlines &lt;- officelines$Line\nlines &lt;- str_remove_all(lines, \"[[:punct:]]\")\n\nofficelines &lt;- officelines |&gt;\nselect(Character, Line, Season, Episode_Number) |&gt;\nmutate(Line = lines) \n\n\nGraph 1 (Who says the most words per season?)\nCleaning data:\n\n# Homogenizing the lines\nwords_per_char &lt;- officelines |&gt;\nmutate(word_count = lengths(str_split(Line, \"\\\\s+\"))) |&gt;\ngroup_by(Character, Season) |&gt;\nsummarize(total_words = sum(word_count))\n\n#Keeping only Main Characters & Cleaning Data\nmain_characters &lt;- c(\"Michael\", \"Jim\", \"Pam\", \"Dwight\", \"Andy\", \"Ryan\", \"Robert\")\n\nwords_dont_keep &lt;- c(\"Voicemail\", \"Mom\", \"Dad\", \"Ad\", \"Fake\", \"Except\", \"Church\")\n\nword_pattern_keep &lt;- paste0(\"\\\\b(\", paste(main_characters, collapse = \"|\"), \")\\\\b\")\n\nword_pattern_delete &lt;- paste0(\"\\\\b(\", paste(words_dont_keep, collapse = \"|\"), \")\\\\b\")\n\nget_rid_ands &lt;- \"\\\\s&\\\\s|\\\\sAnd\\\\s|/|,\\\\s\"\n\nwords_per_char &lt;- words_per_char |&gt;\nfilter(str_detect(Character, word_pattern_keep)) |&gt;\nfilter(!str_detect(Character, word_pattern_delete)) |&gt;\nmutate(Character = str_replace_all(Character, get_rid_ands, \", \")) |&gt;\nseparate_rows(Character, sep = \", \") |&gt;\nfilter(str_detect(Character, word_pattern_keep)) |&gt;\nmutate(Character = case_when(\nstr_detect(Character, \"\\\\bJim\\\\b\") ~ \"Jim\",\nstr_detect(Character, \"\\\\bPam\\\\b\") ~ \"Pam\",\nstr_detect(Character, \"\\\\bDwight\\\\b\") ~ \"Dwight\",\nstr_detect(Character, \"\\\\bMichael\\\\b\") ~ \"Michael\",\nstr_detect(Character, \"\\\\bAndy\\\\b\") ~ \"Andy\",\nstr_detect(Character, \"\\\\bRobert\\\\b\") ~ \"Robert\", str_detect(Character, \"\\\\bRyan\\\\b\") ~ \"Ryan\")) |&gt;\ngroup_by(Character, Season) |&gt;\nsummarise(total_words = sum(total_words), .groups = 'drop') \n\n\n\nFinally! Graphing!!!\n\nggplot(words_per_char, aes(x = Character, y = total_words, fill = Character)) +\ngeom_bar(stat = \"identity\") +\nfacet_wrap(~Season) +\ntheme_minimal() +\nlabs(\ntitle = \"Who says the most words in The Office?\",\nsubtitle = \"Main characters selected via The Office Wiki\",\nx = \"Season\", \ny = \"Total Words Spoken\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nThe figure shows the total number of words spoken by each of the main characters (Andy, Dwight, Jim, Michael, Pam, Robert, and Ryan) over the course of a season. Though this data visualization, you can see that Michael is the character who speaks the most (word-wise) throughout seasons 1-7. He was not present in season 8 & 9 (denoted by a lack of words). The character with the second most words throughout seasons 1-8 (except for season 4) is Dwight, who ends up having the most words in season 9. In season 8, Andy has the most words. On average Robert has no words per season, except for a slight amount of words in season 7 and more words in season 8. Ryan also has very few words throughout the show but is consistently present unlike Robert. The legend shows a color-coded labeling system for each of the main character. The x-axis denotes the Season (with number labeling shown above the graphs) and the y-axis shows the total words spoken.\n\n\nGraph 2 (What percentage of the show involves cussing?)\n\n# Working on the data\ntotal_words &lt;- officelines |&gt;\n  mutate(word_count = lengths(str_split(Line, \"\\\\s+\"))) |&gt;\n  summarize(total_words = sum(word_count))\n\n\nbad_words &lt;- c(\"fuck\", \"bitch\", \"shit\", \"mother-fucker\", \"mother fucker\", \"dumb-ass\", \"dumb ass\", \"dumbass\", \"god damn\", \"goddamn\", \"god dammit\", \"goddammit\", \"dick-head\", \"dickhead\", \"goddamned\", \"whore\", \"slut\", \"ass\", \"bastard\", \"bullshit\", \"damned\", \"damn\", \"dick\", \"hell\", \"prick\", \"piss\", \"wanker\", \"retard\", \"crap\", \"douche\")\n\nbad_words_pattern &lt;- paste0(\"\\\\b(\", paste(bad_words, collapse = \"|\"), \")\\\\b\")\n\nofficelines &lt;- officelines |&gt;\n  filter(!is.na(Line)) |&gt;\n  rowwise() |&gt;\n  mutate(bad_word_count = sum(str_count(Line, bad_words_pattern))) |&gt;\n    ungroup()\n\nlst_of_bw_count &lt;- officelines$bad_word_count\ntotal_bad_words &lt;- sum(lst_of_bw_count)\n\nlst_total_words &lt;- total_words$total_words\n\nwordsdf &lt;- data.frame(\n  totalwords = lst_total_words,\n  unfriendly_words = total_bad_words\n)\n\nwordsdf &lt;- wordsdf |&gt;\n  mutate(bad_words_perc = unfriendly_words/totalwords * 100) |&gt;\n  mutate(good_words_perc = (100 - bad_words_perc))\n\n# Graphing\n\nslices &lt;- c(wordsdf$good_words_perc, wordsdf$bad_words_perc)\nlabels &lt;- c(\"Family Friendly Words, 99.9%\", \"Bad Words, ~0.1%\")\npie(slices, labels = labels, main = \"Percentage of Bad Words Used in The Office\", col = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\nThis graph clearly shows the lack of swear words in proportion to the rest of the show dialogue throughout the 9 seasons, with swear words only making up about 1% of the show’s total words. This show is rated PG-13, which may explain the lack of cussing and an under-reliance of potty-mouth humor. Blue denotes the family-friendly words, and red (although not very visible) displays the swear words. Percentage labeling is included next to the slice tick-marks to aid with quantifying percentages."
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About",
    "section": "",
    "text": "I am currently working on a research manuscript for my senior thesis, which I hope to publish upon completion. My project focuses on investigating differential microbial assemblages in the soil beneath five native California sagescrub plant species. This research explores the microbial diversity unique to each plant species and investigates how proximity to urban boundaries—an aspect known as the “edge effect”—influences the composition of these microbial communities. By examining edge and central soil samples, I aim to uncover how human-influenced environmental factors might alter microbial ecosystems in these natural settings.\nTo achieve this, I am conducting a bioinformatics analysis on the genomic data of both fungal and bacterial assemblages. Working with tools such as Jupyter Notebook and Python, I process and analyze large genomic datasets to identify patterns and draw meaningful insights. This analysis involves a variety of computational techniques, including data cleaning, genomic sequencing alignment, and community composition visualization, providing me with hands-on experience in advanced bioinformatics workflows."
  },
  {
    "objectID": "about.html#research-experience",
    "href": "about.html#research-experience",
    "title": "About",
    "section": "",
    "text": "University of Melbourne Bioinformatics Research Project\nAs a research intern, I immersed myself in a project that examined gene expression patterns in Schistosoma haematobium, a parasitic organism with significant health impacts in endemic regions. This experience challenged me to bridge my biological knowledge with computational analysis, as I developed and implemented over 12 statistical models in R to interpret and visualize genomic data for over 10,000 sequences. I used the HISAT2 alignment program on the Galaxy platform to map gene expression levels across sex chromosomes, an innovative approach that broadened my understanding of bioinformatics workflows. Working alongside graduate and Ph.D. researchers, I gained firsthand experience in interdisciplinary collaboration, where I learned to refine protocols, troubleshoot data issues, and maintain data accuracy across large datasets. Presenting our findings at an international symposium was a transformative moment, teaching me to distill complex data insights into accessible, impactful narratives for a diverse scientific audience.\nSummer Undergraduate Research Program at Pomona College\nThis research experience was pivotal in expanding my understanding of ecological research design and data management. I led the sample collection and protocol design for a project exploring ecological interactions across five Southern Californian ecosystems, emphasizing precision in data collection to minimize variance across sites. I was responsible for transforming raw field data into a meaningful story that could contribute to the scientific community’s understanding of these unique environments. My contribution to our publication in Diversity included not only statistical analysis but also creating detailed data visualizations that underscored key ecological patterns and implications. Presenting our findings at academic conferences reinforced my ability to communicate scientific data visually and verbally, allowing me to advocate for ecological preservation through accessible, evidence-based presentations.\nRobert J. Bernard Field Station Research\nAt the Bernard Field Station, I engaged deeply with research on plant-soil feedback mechanisms and the ecological impact of native and invasive species, a project that aligned with my passion for biodiversity conservation. Working as a Restoration Technician, I conducted methodical data collection across 12 plots, analyzed microbial interactions, and tested soil nutrient levels. This experience brought home the importance of rigorous, repeatable research practices in ecological restoration, where every detail impacts our understanding of ecosystem health. Our team’s publication illuminated the complex interdependencies between plants, soil, and microbial communities, with my contributions to data visualization capturing the intricate relationships that promote ecosystem resilience. This project was instrumental in teaching me the real-world applications of ecological research, from laboratory techniques like DNA extraction to statistical methods that unveil hidden patterns in complex ecological datasets."
  },
  {
    "objectID": "about.html#laboratory-experience",
    "href": "about.html#laboratory-experience",
    "title": "About",
    "section": "",
    "text": "Cell Chemistry and Cell Biology Lab Teaching Assistant\n\nAs a Laboratory Teaching Assistant, I guided over 30 students in mastering core techniques of data analysis, visualization, and lab procedures. In this role, I emphasized precision and analytical thinking, helping students transform raw data into clear, scientifically sound reports. Leading hands-on sessions, I taught methods like western blotting, gel electrophoresis, and spectroscopy, instilling in students the importance of careful data handling. By fostering collaborative learning and critical problem-solving, I helped students build confidence in both their technical skills and their ability to interpret and present data—a foundation essential for future research and professional success.\nSkills: Western blotting, SDS-Page, agarose gel electrophoresis, spectroscopy, PCR, DNA Sequencing, DNA extraction & purification, immunofluorescence, cell culturing, transfection and transformation, knockout techniques, co-immunoprecipitation, spectroscopy, distillation, enzyme kinetic assays"
  },
  {
    "objectID": "about.html#honors-and-scholarships",
    "href": "about.html#honors-and-scholarships",
    "title": "About",
    "section": "",
    "text": "Posse Foundation Full-Tuition Leadership Scholarship (2021 - 2025) 1for2 Foundation “Pay-it-Forward” Scholarship ($14,700 – 2021-2025) Institute for Study Abroad Scholarship—Melbourne, Australia: Academic Year (2023) Taco Bell Live Mas Video Scholarship ($10,000 – 2021; $10,000 – 2023) 2 Published Research Articles (2022 – 2023) Hispanic Scholarship Fund Scholar (2020 – 2023)"
  },
  {
    "objectID": "Mini Project 3.html",
    "href": "Mini Project 3.html",
    "title": "Mini Project 3",
    "section": "",
    "text": "Option A: Conducting Permutation Test\nData was sourced from [Real Estate Valuation](https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set) from UC Irvine Machine Learning Repository by I-Cheng Yeh\nThis dataset shows the historical market of real estate valuation collected from Sindian Dist., New Taipei City, Taiwan. It has many interesting variables to compare–features of homes that may increase or decrease its valuation. In particular, distance from homes to the nearest MRT station interested me, as Taiwan has an extensive public transport system; MRT being one of the most popular.\nAs such, it wouldn’t be a far fetch to believe a home’s distance from an MRT station might increase valuation. As such, we might like to test the correlation between these variables to understand how significant MRT distance truly is to Taiwan’s real estate market.\nResearch Question: Is there a significant association between distance from the nearest MRT station and house price per unit area in Taiwan?\nNull Hypothesis: There is no association between the distance to the nearest MRT station and house price per unit area.\nAlternative Hypothesis: There is an association between the distance to the nearest MRT station and house price per unit area.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Data\nreal_estate &lt;- read_excel(\"C:/Users/anaja/Downloads/Real estate valuation data set.xlsx\")\n\nLet’s start by visualizing. I have started by plotting a graph with the dataset’s original values for MRT distance and their associated price per unit area. Clearly, the slope is pretty negative, perhaps showing that the farther a home is from an MRT station, the less expensive the price per unit area.\nHowever, how much of this is due to random chance? Could it be that all these MRT distance data points lined up just right to show a negative relationship?\nLets try to rearrange the MRT distance data randomly to see if we get the same relationship. To do so, we are sampling the existing MRT distances and assigning them randomly to house price per unit area. As we can see, the linear model shows a much more neutral slope.\n\n# Linear regression of Normal MRT Distance to House Price per Unit Area\nreal_estate |&gt;\n  ggplot(aes(x = distance_MRT, y = house_price_unit)) +\n  geom_point() +\n  labs(x = \"Distance from MRT (meters)\", y = \"House Price of Unit Area\",\n       y = \"House Price of Unit Area\",\n       title = \"MRT Distances to House Price per Unit Area\" ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n# Linear regression of Randomly Sampled MRT Distances to House Price per Unit Area\nset.seed(47)\nreal_estate |&gt;\n    mutate(random_MRT = sample(distance_MRT)) |&gt;\n    ggplot(aes(x = random_MRT, y = house_price_unit)) +\n    geom_point() +\n  labs(x = \"Distance from MRT (meters)\", y = \"House Price of Unit Area\",\n       title = \"Randomly Sampled MRT Distances to House Price per Unit Area\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nBut how much can we really trust our eyes? Lets get some numerical data to back up our claim. Here, we calculate the observed slope’s coefficient to numerically understand how much of a difference we see in slopes.\n\n# Getting the observed slope for original data and plot\nobserved_model &lt;- lm(house_price_unit ~ distance_MRT, data = real_estate)\nobserved_slope &lt;- coef(observed_model)[[\"distance_MRT\"]]\n\nobserved_slope\n\n[1] -0.007262052\n\n# Running Permutation Test with slope for Null Hypothesis\nset.seed(47)\n\npermuted_MRT_prices &lt;- function(real_estate) {\n  real_estate &lt;- real_estate |&gt;\n    mutate(random_MRT = sample(distance_MRT))\n  model_random_MRT &lt;- lm(house_price_unit ~ random_MRT, data = real_estate)\n  return(coef(model_random_MRT)[[\"random_MRT\"]])\n}\n\npermuted_MRT_prices(real_estate)\n\n[1] 0.0005357146\n\n\nOkay, but that was just ONE instance of reshuffling. Maybe this was all chance! Any good scientist knows we should have multiple trials–like 1000 iterations…\n\n# Iterating over multiple random samples of MRT distance\nset.seed(47)\n\nn_permutations &lt;- 1000\nnull_slopes &lt;- map_dbl(1:n_permutations, ~ permuted_MRT_prices(real_estate))\n\naverage_sample_slope &lt;- mean(null_slopes)\n\naverage_sample_slope\n\n[1] 2.056867e-05\n\n\nSo over 1000 permutations, we have an average reshuffled slope of:\n2.056867e-05\nThis is compared to our very negative slope of the original dataset:\n -0.007262052\nLets create a histogram of the null statistics and see if our observed values fall within the histogram:\n\ncomparison_slopes &lt;- tibble(null_slopes, observed_slope)\n\nset.seed(47)\nperm_stats &lt;-\n  map(1:1000, null_slopes, data = comparison_slopes) |&gt;\n  list_rbind()\n\nperm_stats |&gt;\n  ggplot(aes(x = null_slopes)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = observed_slope), color = \"red\") +\n               labs(title = \"Histogram of 1000 Slope Permutations\", \n                    subtitle = \"red line = observed slope (from original data)\", x = \"Permuted Null Slopes\", y = \"Count\")\n\n\n\n\n\n\n\n\nWell, that is interesting…it seems our observed slope (red line) is nowhere near the slopes of calculated in the 1000 permutation. Thats to say, there is likely no way that our observed data would happen by chance–even if we were to run this simulation 1000 times.\nLets also conduct a t-test, just because!\n\nlibrary(broom)\nt_test &lt;- lm(house_price_unit ~ distance_MRT, data = real_estate) |&gt;\n  tidy()\n\nt_test$p.value\n\n[1] 1.856440e-231  4.639825e-56\n\n\nSince our p-values (1.856440 x 10^-231 and 4.639825 x 10^-56, essentially, 0) is much smaller than 0.05, we can confidently reject the null hypothesis–suggesting there is a statistically significant relationship between Taiwanese house prices per unit area and the home’s distance from the MRT. In all, it seems that proximity to MRT stations plays a significant role in Taiwan’s house valuation. The original data set’s negative relationship is shown to be significantly different from the 1000 iterations of permutations tested with randomized data.\nCitation:\nYeh, I. (2018). Real Estate Valuation [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5J30W."
  },
  {
    "objectID": "Mini Project 4.html",
    "href": "Mini Project 4.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "Data was sourced from the [Wideband Acoustic Immittance (WAI) Database](https://www.science.smith.edu/wai-database/) hosted by Smith College\nIn this project, I plan to query the WAI Database to create two sub-queries: one that will allow me to reproduce Figure 1 of the Voss (2020) publication (see image below) and one to allow me to compare mean absorbency of sound among subjects of different races in one of the studies included in the WAI Database. I start by understanding the tables within the dataset and the variables of each relevant table, which is largely done by the DESCRIBE function. I continue by joining datasets with the information relevant to the figures–utilizing primary keys that connect the datasets, such as Identifier and SubjectNumber.\nFigure 1 Voss (2020)\n\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nPART 1: Recreating Voss (2020) Figure 1\nIn this part of my work, I recreated the Voss (2020) Figure 1 as closely as possible by utilizing ggplot2. I began by selecting my relevant variables, and joining via primary keys. I then isolated the data for the studies included in the plot, the desired frequencies, and grouped by the relevant variables. NOTE: Some deviation from the original figure results from updates in the dataset since the figure publication in 2020–4 years of additional data is included.\n\nSELECT \np.Identifier,\np.AuthorsShortList,\np.Year,\nm.Instrument,\nCOUNT(DISTINCT CONCAT(m.SubjectNumber, m.Ear)) AS Unique_Ears,\nm.Frequency, \nAVG(m.Absorbance) AS Mean_Absorbance\nFROM Measurements AS m \nJOIN PI_Info AS p\nON m.Identifier = p.Identifier\nJOIN Subjects AS s ON m.SubjectNumber = s.SubjectNumber\nWHERE p.Identifier IN ('Abur_2014', 'Feeney_2017', 'Groon_2015',\n'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', \n'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', \n'Werner_2010')\nAND m.Frequency BETWEEN 200 AND 8000\nGROUP BY \np.Identifier,\np.AuthorsShortList,\np.Year, m.Instrument,\nm.Frequency\nHAVING\nUnique_Ears &gt; 0\nORDER BY\np.Identifier, m.Frequency;\n\n\nfreq_data |&gt;\n  mutate(Label = paste(AuthorsShortList, \"(\", Year, \") N=\", Unique_Ears, \";\", Instrument)) |&gt;\n  group_by(Label) |&gt;\n  arrange(desc(Label)) |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Label )) +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  scale_x_continuous(limits = c(200,8000), breaks = seq(200, 8000, by = 1000)) +\n  labs(\n    title = \"Mean absorbance from each publication in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\", \n    color = \"Study Details\"\n  ) + \n  theme_minimal()  +\n  scale_color_manual(values = c(\n    \"purple\", \"deepskyblue\", \"aquamarine\", \"darkolivegreen2\", \"darkseagreen1\", \"yellow\", \n    \"khaki2\", \"lemonchiffon2\", \"goldenrod1\", \"orange\", \"sienna2\", \"red\", \"tomato4\"))\n\n\n\n\n\n\n\n\nPART 2: Mean Absorbance of Lewis 2015 Study by Subject Race\nIn this part, I chose to visualize the data of the Lewis 2015 study whose data was collected within the WAI Database. In this, I grouped the data by subject race in visualization, maintaining the previous frequency threshold. I started by creating a SQL subquery for the data I desired, and visualized the mean absorbency to frequency of subjects amongst different racial groups\n\nSELECT \ns.Race, \np.Identifier,\nCOUNT(DISTINCT CONCAT(m.SubjectNumber, m.Ear)) AS Unique_Ears,\nm.Frequency, \nAVG(m.Absorbance) AS Mean_Absorbance\nFROM Measurements AS m \nJOIN PI_Info AS p\nON m.Identifier = p.Identifier\nJOIN Subjects AS s ON m.SubjectNumber = s.SubjectNumber\nWHERE \np.Identifier = ('Lewis_2015')\nAND m.Frequency BETWEEN 200 AND 8000\nGROUP BY s.Race, p.Identifier, m.Frequency\nHAVING\nUnique_Ears &gt; 0;\n\n\nrace_data &lt;- race_data |&gt; \n  mutate(Race = str_trim(Race),   \n         Race = str_to_title(Race)) \n\nrace_data |&gt; \n  group_by(Race) |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Race)) +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  scale_x_continuous(limits = c(200, 8000), breaks = seq(200, 8000, by = 1000)) +\n  labs(\n    title = \"Mean absorbance by Subject Race in Lewis 2015 Data\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\", \n    color = \"Test Subject Race\"\n  ) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nTo conclude, I created subqueries of the WAI Database to best manipulate the data into visualizations using ggplot2. Although the data originated in SQL form, SQL was used to create subqueries that translate into a dataframe output in R. This was then used to create graphs via ggplot2 and tidyverse functions. In the end, a graph mimicking the output of Voss (2020) Figure 1 (with slight, noted deviations) and a graph to visualize another aspect of a specific study, Lewis 2015, differences in mean absorbency at different frequencies among subject race.\nCitation:\nVoss, Susan E., et al. “Wideband Acoustic Immittance Dataset.” Api.Openalex.Org, Smith College, api.openalex.org/works/doi:10.3837/tiis.2022.12.001. Accessed 28 Nov. 2024.\ndoi.org/10.35482/egr.001.2022\nVoss, Susan E. “An Online Wideband Acoustic Immittance (WAI) Database and Corresponding Website.” Ear and Hearing, U.S. National Library of Medicine, 1 Nov. 2020, pmc.ncbi.nlm.nih.gov/articles/PMC7093226/."
  },
  {
    "objectID": "Final Presentation.html#data-analyses-of-the-office",
    "href": "Final Presentation.html#data-analyses-of-the-office",
    "title": "Final Presentation",
    "section": "Data Analyses of The Office",
    "text": "Data Analyses of The Office\nToday, I will be presenting to you (NBC Marketers) some data from The Office that will be used to consider future directions for an upcoming season. With this, we hope to inform show producers’ decisions and increase The Office’s potential viewership."
  },
  {
    "objectID": "Final Presentation.html#the-office-background",
    "href": "Final Presentation.html#the-office-background",
    "title": "Final Presentation",
    "section": "The Office: Background",
    "text": "The Office: Background\n\nDescriptionThe Office is a comedy mockumentary following the lifestyle of seemingly normal office workers at a branch of Dunder Mifflin Paper Company. It is an American show based off the original British version under the same name, adapted for NBC by Greg Daniels."
  },
  {
    "objectID": "Final Presentation.html#the-office-viewership",
    "href": "Final Presentation.html#the-office-viewership",
    "title": "Final Presentation",
    "section": "The Office Viewership",
    "text": "The Office Viewership\nConsiderations:\n\nCharacter Popularity\nLine Designation for Future Seasons\nExisting Trends"
  },
  {
    "objectID": "Final Presentation.html#who-is-the-favorite-character",
    "href": "Final Presentation.html#who-is-the-favorite-character",
    "title": "Final Presentation",
    "section": "Who is the Favorite Character?",
    "text": "Who is the Favorite Character?\nRanker.com\nA platform for user-generated rankings on various topics, including entertainment.\n\nIt seems Michael Scott has the public vote."
  },
  {
    "objectID": "Final Presentation.html#method-for-main-character-determination",
    "href": "Final Presentation.html#method-for-main-character-determination",
    "title": "Final Presentation",
    "section": "Method for Main Character Determination",
    "text": "Method for Main Character Determination\nAs the show follows multiple characters, there is some debate as to who exactly the main character is.\n\nMethod:\n\nCount the amount of words spoken by each character per season\nUnderstand who the show writers center most"
  },
  {
    "objectID": "Final Presentation.html#data",
    "href": "Final Presentation.html#data",
    "title": "Final Presentation",
    "section": "Data",
    "text": "Data\n\n# Data \nlibrary(tidyverse)\n\nofficelines &lt;- read_csv(\"the-office_lines.csv\")  \nhead(officelines)\n\n# A tibble: 6 × 5\n   ...1 Character Line                                     Season Episode_Number\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                                     &lt;dbl&gt;          &lt;dbl&gt;\n1     0 Michael   All right Jim. Your quarterlies look ve…      1              1\n2     1 Jim       Oh, I told you. I couldn’t close it. So…      1              1\n3     2 Michael   So you’ve come to the master for guidan…      1              1\n4     3 Jim       Actually, you called me in here, but ye…      1              1\n5     4 Michael   All right. Well, let me show you how it…      1              1\n6     5 Michael   [on the phone] Yes, I’d like to speak t…      1              1"
  },
  {
    "objectID": "Final Presentation.html#homogenizing-line-data",
    "href": "Final Presentation.html#homogenizing-line-data",
    "title": "Final Presentation",
    "section": "Homogenizing Line Data",
    "text": "Homogenizing Line Data\n  \n\n# Cleaning Data\nofficelines &lt;- officelines |&gt; \n  mutate(Line = str_to_lower(Line))  \nlines &lt;- officelines$Line \nlines &lt;- str_remove_all(lines, \"[[:punct:]]\")  \nofficelines &lt;- officelines |&gt; \n  select(Character, Line, Season, Episode_Number) |&gt; \n  mutate(Line = lines)   \n\n# Homogenizing the lines \nwords_per_char &lt;- officelines |&gt; \n  mutate(word_count = lengths(str_split(Line, \"\\\\s+\"))) |&gt; \n  group_by(Character, Season) |&gt; \n  summarize(total_words = sum(word_count))"
  },
  {
    "objectID": "Final Presentation.html#most-relevant-characters",
    "href": "Final Presentation.html#most-relevant-characters",
    "title": "Final Presentation",
    "section": "Most Relevant Characters",
    "text": "Most Relevant Characters\n\nSelected via The Office Wiki"
  },
  {
    "objectID": "Final Presentation.html#cleaning-character-list",
    "href": "Final Presentation.html#cleaning-character-list",
    "title": "Final Presentation",
    "section": "Cleaning Character List",
    "text": "Cleaning Character List\n\ndistinct_char &lt;- words_per_char |&gt;\n  distinct(Character)\n\ndistinct_char\n\n# A tibble: 776 × 1\n# Groups:   Character [776]\n   Character           \n   &lt;chr&gt;               \n 1 (Pam’S Mom) Heleen  \n 2 3Rd Athlead Employee\n 3 4Th Athlead Employee\n 4 A.J.                \n 5 Aaron Rodgers       \n 6 Abby                \n 7 Abe                 \n 8 Actor               \n 9 Actress             \n10 Ad Guy 1            \n# ℹ 766 more rows\n\n\nCharacters like “Pam’s Mom” need to be removed! We should also should make sure to count things said by main characters together."
  },
  {
    "objectID": "Final Presentation.html#filtering-for-most-relevant-characters",
    "href": "Final Presentation.html#filtering-for-most-relevant-characters",
    "title": "Final Presentation",
    "section": "Filtering for Most Relevant Characters",
    "text": "Filtering for Most Relevant Characters\n\n#Keeping only Main Characters & Cleaning Data\nmain_characters &lt;- c(\"Michael\", \"Jim\", \"Pam\", \"Dwight\", \"Andy\", \"Ryan\", \"Robert\")\n\nwords_dont_keep &lt;- c(\"Voicemail\", \"Mom\", \"Dad\", \"Ad\", \"Fake\", \"Except\", \"Church\")\n\nword_pattern_keep &lt;- paste0(\"\\\\b(\", paste(main_characters, collapse = \"|\"), \")\\\\b\")\n\nword_pattern_delete &lt;- paste0(\"\\\\b(\", paste(words_dont_keep, collapse = \"|\"), \")\\\\b\")\n\nget_rid_ands &lt;- \"\\\\s&\\\\s|\\\\sAnd\\\\s|/|,\\\\s\"\n\nwords_per_char &lt;- words_per_char |&gt;\nfilter(str_detect(Character, word_pattern_keep)) |&gt;\nfilter(!str_detect(Character, word_pattern_delete)) |&gt;\nmutate(Character = str_replace_all(Character, get_rid_ands, \", \")) |&gt;\nseparate_rows(Character, sep = \", \") |&gt;\nfilter(str_detect(Character, word_pattern_keep)) |&gt;\nmutate(Character = case_when(\nstr_detect(Character, \"\\\\bJim\\\\b\") ~ \"Jim\",\nstr_detect(Character, \"\\\\bPam\\\\b\") ~ \"Pam\",\nstr_detect(Character, \"\\\\bDwight\\\\b\") ~ \"Dwight\",\nstr_detect(Character, \"\\\\bMichael\\\\b\") ~ \"Michael\",\nstr_detect(Character, \"\\\\bAndy\\\\b\") ~ \"Andy\",\nstr_detect(Character, \"\\\\bRobert\\\\b\") ~ \"Robert\", \nstr_detect(Character, \"\\\\bRyan\\\\b\") ~ \"Ryan\")) |&gt;\ngroup_by(Character, Season) |&gt;\nsummarise(total_words = sum(total_words), .groups = 'drop')"
  },
  {
    "objectID": "Final Presentation.html#final-dataset",
    "href": "Final Presentation.html#final-dataset",
    "title": "Final Presentation",
    "section": "Final Dataset",
    "text": "Final Dataset\n\nwords_per_char\n\n# A tibble: 55 × 3\n   Character Season total_words\n   &lt;chr&gt;      &lt;dbl&gt;       &lt;int&gt;\n 1 Andy           3        4867\n 2 Andy           4        2924\n 3 Andy           5        5624\n 4 Andy           6        6776\n 5 Andy           7        6611\n 6 Andy           8       14452\n 7 Andy           9        9157\n 8 Dwight         1        3699\n 9 Dwight         2       11895\n10 Dwight         3       10226\n# ℹ 45 more rows"
  },
  {
    "objectID": "Final Presentation.html#data-visualization",
    "href": "Final Presentation.html#data-visualization",
    "title": "Final Presentation",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nggplot(words_per_char, aes(x = Character, y = total_words, fill = Character)) +\ngeom_bar(stat = \"identity\") +\nfacet_wrap(~Season) +\ntheme_minimal() +\nlabs(\ntitle = \"Who says the most words in The Office?\",\nsubtitle = \"Main characters selected via The Office Wiki\",\nx = \"Season\", \ny = \"Total Words Spoken\") +\n  theme(axis.text.x = element_blank())"
  },
  {
    "objectID": "Final Presentation.html#takeaways",
    "href": "Final Presentation.html#takeaways",
    "title": "Final Presentation",
    "section": "Takeaways",
    "text": "Takeaways\n\nMichael Scott’s Dominance in Word Count:\n\nMichael consistently speaks the most words across all seasons where he appears.\n\nDrop in Michael’s Words After Season 7:\n\nThere is a sharp decline in Michael’s dialogue starting in Season 8 (after his departure).\nOther characters, such as Andy, Dwight, and Jim, seem to fill the void, but none fully replace his prominence.\n\nIncreased Focus on Other Characters in Later Seasons:\n\nAndy’s role expands significantly in Seasons 8 and 9.\nJim and Dwight play a strong secondary role."
  },
  {
    "objectID": "Final Presentation.html#main-character-and-trend",
    "href": "Final Presentation.html#main-character-and-trend",
    "title": "Final Presentation",
    "section": "Main Character and Trend",
    "text": "Main Character and Trend\nTo ensure the success of Season 10, producers should strongly consider bringing Michael Scott back as a central character. Michael Scott was not only a pivotal figure in the show’s narrative but also a driving force behind its immense popularity.\nReinstating Michael Scott as the main character would tap into the nostalgia of long-time fans while reigniting interest among lapsed viewers. By leveraging his role as the heart of the show, NBC can create a revitalized narrative that resonates with both loyal fans and new audiences, ensuring sustained and even increased viewership for the series."
  },
  {
    "objectID": "Final Presentation.html#the-office",
    "href": "Final Presentation.html#the-office",
    "title": "Final Presentation",
    "section": "The Office",
    "text": "The Office\n\nBackgroundThe Office is a comedy mockumentary following the lifestyle of seemingly normal office workers at a branch of Dunder Mifflin Paper Company. It is an American show based off the original British version under the same name, adapted for NBC by Greg Daniels."
  },
  {
    "objectID": "Final Presentation.html#homogenizing-total-line-data",
    "href": "Final Presentation.html#homogenizing-total-line-data",
    "title": "Final Presentation",
    "section": "Homogenizing Total Line Data",
    "text": "Homogenizing Total Line Data\n\n# Cleaning Data\nofficelines &lt;- officelines |&gt;\nmutate(Line = str_to_lower(Line))\n\nlines &lt;- officelines$Line\nlines &lt;- str_remove_all(lines, \"[[:punct:]]\")\n\nofficelines &lt;- officelines |&gt;\nselect(Character, Line, Season, Episode_Number) |&gt;\nmutate(Line = lines) \n\n# Counting the words\nwords_per_char &lt;- officelines |&gt;\nmutate(word_count = lengths(str_split(Line, \"\\\\s+\"))) |&gt;\ngroup_by(Character, Season) |&gt;\nsummarize(total_words = sum(word_count))\n\n\nlowercase\nremove punctuation\nsum words said by character in a season"
  }
]